{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and Cost Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y, yhat):\n",
    "    yloss = -1 * (((1-y) * np.log(1-yhat)) + (y * np.log(yhat)))\n",
    "    return yloss   \n",
    "\n",
    "def cost(Y, Yhat):\n",
    "    Jcost = np.sum(loss(Y, Yhat))/Y.shape[0]\n",
    "    return Jcost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unit_activation(yhat):    \n",
    "    return yhat\n",
    "unit_activation = np.vectorize(unit_activation)\n",
    "\n",
    "def sigmoid_activation(yhat):    \n",
    "    return 1/(1+np.exp(-yhat))\n",
    "sigmoid_activation = np.vectorize(sigmoid_activation)\n",
    "\n",
    "def tanh_activation(yhat):    \n",
    "    return (np.exp(yhat) - np.exp(-yhat)) / (np.exp(yhat) + np.exp(-yhat)) \n",
    "tanh_activation = np.vectorize(tanh_activation)\n",
    "\n",
    "def tanh_activation_deriv(yhat):    \n",
    "    return 1 - tanh_activation(yhat)**2\n",
    "tanh_activation_deriv = np.vectorize(tanh_activation_deriv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Matrices for Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features:  2\n",
      "Number of Training Examples:  320\n",
      "Number of Test Examples:  80\n"
     ]
    }
   ],
   "source": [
    "X = np.genfromtxt('pattern_rand.csv',delimiter=',', skip_header=True)[0:320,0:2].T \n",
    "Y = np.genfromtxt('pattern_rand.csv',delimiter=',', skip_header=True)[0:320,2:3].T \n",
    "\n",
    "Xt = np.genfromtxt('pattern_rand.csv',delimiter=',', skip_header=True)[320:400,0:2].T \n",
    "Yt = np.genfromtxt('pattern_rand.csv',delimiter=',', skip_header=True)[320:400,2:3].T \n",
    "Yb = np.genfromtxt('pattern_rand.csv',delimiter=',', skip_header=True)[:,2:3].T \n",
    "    \n",
    "n = X.shape[0]\n",
    "m = X.shape[1]\n",
    "t = Xt.shape[1]\n",
    "\n",
    "print(\"Number of Features: \", n)\n",
    "print(\"Number of Training Examples: \", m)\n",
    "print(\"Number of Test Examples: \", t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization and Running Forward Propagation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iterations\n",
    "h = 1000\n",
    "alpha = 0.05\n",
    "# Initialization of J and Yhat\n",
    "J = np.zeros((1,h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jcost = 0.6636478308255713\n",
      "B2 = [[0.34498125]]\n",
      "W2 =  [[1.07094191 1.28574992 0.63743756 0.63743756]]\n",
      "B1 = [[ 0.10271366]\n",
      " [-0.18012133]\n",
      " [-0.16543166]\n",
      " [-0.16543166]]\n",
      "W1 =  [[ 1.46444322 -1.47307305]\n",
      " [ 0.03417125  2.07193255]\n",
      " [-1.20147522 -1.04187516]\n",
      " [-1.20147522 -1.04187516]]\n",
      "cost_sample =  [1.5921388658356355, 0.6704962029732322, 0.664431508034775, 0.6638940471347393, 0.6637784531349715, 0.6637247253054398, 0.6636940653610119, 0.6636749159546148, 0.6636623401435741, 0.6636537960207154]\n"
     ]
    }
   ],
   "source": [
    "# IL0: Number of inputs to Layer 0\n",
    "# NL0: Number of neurons in Layer 0\n",
    "IL0 = n\n",
    "NL0 = n\n",
    "# The first layer is input layer so both number of inputs \n",
    "# and number of neurons are equal to number of features\n",
    "\n",
    "# IL1: Number of inputs to Layer 1\n",
    "# NL1: Number of neurons in Layer 1\n",
    "IL1 = NL0\n",
    "NL1 = 4\n",
    "# As there are n neurons in the previous layer, the number\n",
    "# of inputs to Layer 1 is equal to n. Number of neurons\n",
    "# may be any. \n",
    "\n",
    "# IL2: Number of inputs to Layer 2\n",
    "# NL2: Number of neurons in Layer 2\n",
    "IL2 = NL1\n",
    "NL2 = 1\n",
    "# Number of inputs in Layer 2 equals to number of neurons in\n",
    "# the previous layer (one input coming from each neuron). As \n",
    "# this is binary classification with a single output, the number \n",
    "# of neurons equals 1. \n",
    "\n",
    "\n",
    "# Layer 0 is input layer so W0 is identity matrix while B0 is zero matrix.\n",
    "# Layer 0 in a way simply reproduces its inputs at the output\n",
    "B0 = np.zeros((NL0, 1))\n",
    "W0 = np.identity((NL0))\n",
    "\n",
    "# Layer 1 is a passthrough layer. As number of inputs to the layer equals\n",
    "# 2 and number of neurons are 4, the size of W1 will be 4 x2. To make this\n",
    "# layer a passthrough layer, the 4 x 2 W1 matrix needs to consist of a \n",
    "# a 2 x 2 identity matrix and a 2 x 2 zero matrix. Also B1 is a zero matrix\n",
    "B1 = np.zeros((NL1,1))\n",
    "W1 = np.array([[1, 0],[0, 1],[0, 0],[0, 0]])\n",
    "\n",
    "\n",
    "# Layer 2 is a simple Logistic Regression Unit. W2 and B2 are intialized as ones.\n",
    "B2 = np.ones((NL2,1))\n",
    "W2 = np.ones((NL2,IL2))\n",
    "\n",
    "cost_sample = []\n",
    "\n",
    "# We now runs a loop for performing forward- and backpropagation\n",
    "for g in range(h):\n",
    "    # Forward propagation \n",
    "    X0 = X\n",
    "    G0 = np.matmul(W0,X0) + B0\n",
    "    H0 = unit_activation(G0)\n",
    "    X1 = H0\n",
    "    G1 = np.matmul(W1,X1) + B1\n",
    "    H1 = tanh_activation(G1)\n",
    "    X2 = G1\n",
    "    G2 = np.matmul(W2,X2) + B2\n",
    "    H2 = sigmoid_activation(G2)\n",
    "    Yhat = H2\n",
    "\n",
    "    # Determine Cost\n",
    "    J[0,g] = cost(Y, Yhat)/m\n",
    "    \n",
    "    if (g % 100 == 0):\n",
    "        cost_sample.append(J[0,g])\n",
    "    \n",
    "    # Backpropagation Layer 2\n",
    "    dJdG2 = Yhat - Y\n",
    "    dJdB2 =  np.sum(dJdG2, axis=1,keepdims=True)/m \n",
    "    dJdW2 = np.matmul(dJdG2, X2.T)/m\n",
    "    B2 = B2 - alpha * dJdB2\n",
    "    W2 = W2 - alpha * dJdW2\n",
    "    \n",
    "    # Backpropagation Layer 1\n",
    "    dG1dW1 = np.matmul(W2.T, dJdG2) * tanh_activation_deriv(G1)\n",
    "    dJdW1 = np.matmul(dG1dW1, X1.T)/m\n",
    "    dJdB1 = np.matmul(dG1dW1, np.ones((m,1)))/m\n",
    "    B1 = B1 - alpha * dJdB1\n",
    "    W1 = W1 - alpha * dJdW1\n",
    "    \n",
    "X0 = X\n",
    "G0 = np.matmul(W0,X0) + B0\n",
    "H0 = unit_activation(G0)\n",
    "X1 = H0\n",
    "G1 = np.matmul(W1,X1) + B1\n",
    "H1 = tanh_activation(G1)\n",
    "X2 = G1\n",
    "Yhat = np.dot(W2,X2) + B2   \n",
    "Yhat = sigmoid_activation(Yhat)\n",
    "J[0,g] = np.sum(loss(Y, Yhat))/m\n",
    "print(\"Jcost = {}\".format(J[0,h-1]))\n",
    "print(\"B2 = {}\".format(B2))\n",
    "print(\"W2 = \", W2)\n",
    "print(\"B1 = {}\".format(B1))\n",
    "print(\"W1 = \", W1)\n",
    "print(\"cost_sample = \",cost_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(J[0, 0:h-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forward propagation (Test data)\n",
    "X0 = Xt\n",
    "G0 = np.matmul(W0,X0) + B0\n",
    "H0 = unit_activation(G0)\n",
    "X1 = H0\n",
    "G1 = np.matmul(W1,X1) + B1\n",
    "H1 = tanh_activation(G1)\n",
    "X2 = G1\n",
    "G2 = np.matmul(W2,X2) + B2\n",
    "H2 = sigmoid_activation(G2)\n",
    "Ythat = H2\n",
    "\n",
    "Ythis = []\n",
    "\n",
    "for i in range(Xt.shape[1]):\n",
    "    if Ythat[0,i] >= 0.5:\n",
    "        Ythis.append(1.0)\n",
    "    else:\n",
    "        Ythis.append(0.0)\n",
    "Ythis = np.array(Ythis).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 46.25%\n"
     ]
    }
   ],
   "source": [
    "total = Yt.shape[1]\n",
    "error = np.sum(abs(Yt - Ythis))\n",
    "print(\"Accuracy: {}%\".format((total-error)*100/total))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
