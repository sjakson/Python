{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and Cost Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y, yhat):\n",
    "    yloss = -1 * (((1-y) * np.log(1-yhat)) + (y * np.log(yhat)))\n",
    "    return yloss   \n",
    "\n",
    "def cost(Y, Yhat):\n",
    "    Jcost = np.sum(loss(Y, Yhat))/Y.shape[0]\n",
    "    return Jcost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unit_activation(yhat):    \n",
    "    return yhat\n",
    "unit_activation = np.vectorize(unit_activation)\n",
    "\n",
    "def sigmoid_activation(yhat):    \n",
    "    return 1/(1+np.exp(-yhat))\n",
    "sigmoid_activation = np.vectorize(sigmoid_activation)\n",
    "\n",
    "def relu_activation(yhat):    \n",
    "    return yhat * (yhat > 0)\n",
    "relu_activation = np.vectorize(relu_activation)\n",
    "\n",
    "def relu_activation_deriv(yhat):    \n",
    "    return np.sign(np.maximum(0,yhat))\n",
    "relu_activation_deriv = np.vectorize(relu_activation_deriv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Matrices for Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features:  2\n",
      "Number of Training Examples:  320\n",
      "Number of Test Examples:  80\n"
     ]
    }
   ],
   "source": [
    "X = np.genfromtxt('pattern_rand.csv',delimiter=',', skip_header=True)[0:320,0:2].T \n",
    "Y = np.genfromtxt('pattern_rand.csv',delimiter=',', skip_header=True)[0:320,2:3].T \n",
    "\n",
    "Xt = np.genfromtxt('pattern_rand.csv',delimiter=',', skip_header=True)[320:400,0:2].T \n",
    "Yt = np.genfromtxt('pattern_rand.csv',delimiter=',', skip_header=True)[320:400,2:3].T \n",
    "Yb = np.genfromtxt('pattern_rand.csv',delimiter=',', skip_header=True)[:,2:3].T \n",
    "    \n",
    "n = X.shape[0]\n",
    "m = X.shape[1]\n",
    "t = Xt.shape[1]\n",
    "\n",
    "print(\"Number of Features: \", n)\n",
    "print(\"Number of Training Examples: \", m)\n",
    "print(\"Number of Test Examples: \", t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization and Running Forward Propagation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iterations\n",
    "h = 1000\n",
    "alpha = 0.05\n",
    "# Initialization of J and Yhat\n",
    "J = np.zeros((1,h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jcost = 0.6649768117834663\n",
      "B2 = [[-0.18887031]]\n",
      "W2 =  [[0.72019485 0.4670301  1.         1.        ]]\n",
      "B1 = [[ 0.6834331 ]\n",
      " [-0.63324905]\n",
      " [ 0.        ]\n",
      " [ 0.        ]]\n",
      "W1 =  [[ 0.51406801 -0.56166798]\n",
      " [-0.53949598  0.42692925]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]]\n",
      "cost_sample =  [1.5921388658356355, 0.6958216831935922, 0.6728987955435611, 0.6663022591112616, 0.665614206021298, 0.6662008896739277, 0.6659382461958542, 0.6658941601332252, 0.6656228752472518, 0.6652592266207706]\n"
     ]
    }
   ],
   "source": [
    "# IL0: Number of inputs to Layer 0\n",
    "# NL0: Number of neurons in Layer 0\n",
    "IL0 = n\n",
    "NL0 = n\n",
    "# The first layer is input layer so both number of inputs \n",
    "# and number of neurons are equal to number of features\n",
    "\n",
    "# IL1: Number of inputs to Layer 1\n",
    "# NL1: Number of neurons in Layer 1\n",
    "IL1 = NL0\n",
    "NL1 = 4\n",
    "# As there are n neurons in the previous layer, the number\n",
    "# of inputs to Layer 1 is equal to n. Number of neurons\n",
    "# may be any. \n",
    "\n",
    "# IL2: Number of inputs to Layer 2\n",
    "# NL2: Number of neurons in Layer 2\n",
    "IL2 = NL1\n",
    "NL2 = 1\n",
    "# Number of inputs in Layer 2 equals to number of neurons in\n",
    "# the previous layer (one input coming from each neuron). As \n",
    "# this is binary classification with a single output, the number \n",
    "# of neurons equals 1. \n",
    "\n",
    "\n",
    "# Layer 0 is input layer so W0 is identity matrix while B0 is zero matrix.\n",
    "# Layer 0 in a way simply reproduces its inputs at the output\n",
    "B0 = np.zeros((NL0, 1))\n",
    "W0 = np.identity((NL0))\n",
    "\n",
    "# Layer 1 is a passthrough layer. As number of inputs to the layer equals\n",
    "# 2 and number of neurons are 4, the size of W1 will be 4 x2. To make this\n",
    "# layer a passthrough layer, the 4 x 2 W1 matrix needs to consist of a \n",
    "# a 2 x 2 identity matrix and a 2 x 2 zero matrix. Also B1 is a zero matrix\n",
    "B1 = np.zeros((NL1,1))\n",
    "W1 = np.array([[1, 0],[0, 1],[0, 0],[0, 0]])\n",
    "\n",
    "\n",
    "# Layer 2 is a simple Logistic Regression Unit. W2 and B2 are intialized as ones.\n",
    "B2 = np.ones((NL2,1))\n",
    "W2 = np.ones((NL2,IL2))\n",
    "\n",
    "cost_sample = []\n",
    "\n",
    "# We now runs a loop for performing forward- and backpropagation\n",
    "for g in range(h):\n",
    "    # Forward propagation \n",
    "    X0 = X\n",
    "    G0 = np.matmul(W0,X0) + B0\n",
    "    H0 = unit_activation(G0)\n",
    "    X1 = H0\n",
    "    G1 = np.matmul(W1,X1) + B1\n",
    "    H1 = relu_activation(G1)\n",
    "    X2 = G1\n",
    "    G2 = np.matmul(W2,X2) + B2\n",
    "    H2 = sigmoid_activation(G2)\n",
    "    Yhat = H2\n",
    "\n",
    "    # Determine Cost\n",
    "    J[0,g] = cost(Y, Yhat)/m\n",
    "    \n",
    "    if (g % 100 == 0):\n",
    "        cost_sample.append(J[0,g])\n",
    "    \n",
    "    # Backpropagation Layer 2\n",
    "    dJdG2 = Yhat - Y\n",
    "    dJdB2 =  np.sum(dJdG2, axis=1,keepdims=True)/m \n",
    "    dJdW2 = np.matmul(dJdG2, X2.T)/m\n",
    "    B2 = B2 - alpha * dJdB2\n",
    "    W2 = W2 - alpha * dJdW2\n",
    "    \n",
    "    # Backpropagation Layer 1\n",
    "    dG1dW1 = np.matmul(W2.T, dJdG2) * relu_activation_deriv(G1)\n",
    "    dJdW1 = np.matmul(dG1dW1, X1.T)/m\n",
    "    dJdB1 = np.matmul(dG1dW1, np.ones((m,1)))/m\n",
    "    B1 = B1 - alpha * dJdB1\n",
    "    W1 = W1 - alpha * dJdW1\n",
    "    \n",
    "X0 = X\n",
    "G0 = np.matmul(W0,X0) + B0\n",
    "H0 = unit_activation(G0)\n",
    "X1 = H0\n",
    "G1 = np.matmul(W1,X1) + B1\n",
    "H1 = relu_activation(G1)\n",
    "X2 = G1\n",
    "Yhat = np.dot(W2,X2) + B2   \n",
    "Yhat = sigmoid_activation(Yhat)\n",
    "J[0,g] = np.sum(loss(Y, Yhat))/m\n",
    "print(\"Jcost = {}\".format(J[0,h-1]))\n",
    "print(\"B2 = {}\".format(B2))\n",
    "print(\"W2 = \", W2)\n",
    "print(\"B1 = {}\".format(B1))\n",
    "print(\"W1 = \", W1)\n",
    "print(\"cost_sample = \",cost_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(J[0, 0:h-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forward propagation (Test data)\n",
    "X0 = Xt\n",
    "G0 = np.matmul(W0,X0) + B0\n",
    "H0 = unit_activation(G0)\n",
    "X1 = H0\n",
    "G1 = np.matmul(W1,X1) + B1\n",
    "H1 = relu_activation(G1)\n",
    "X2 = G1\n",
    "G2 = np.matmul(W2,X2) + B2\n",
    "H2 = sigmoid_activation(G2)\n",
    "Ythat = H2\n",
    "\n",
    "Ythis = []\n",
    "\n",
    "for i in range(Xt.shape[1]):\n",
    "    if Ythat[0,i] >= 0.5:\n",
    "        Ythis.append(1.0)\n",
    "    else:\n",
    "        Ythis.append(0.0)\n",
    "Ythis = np.array(Ythis).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 47.5%\n"
     ]
    }
   ],
   "source": [
    "total = Yt.shape[1]\n",
    "error = np.sum(abs(Yt - Ythis))\n",
    "print(\"Accuracy: {}%\".format((total-error)*100/total))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
